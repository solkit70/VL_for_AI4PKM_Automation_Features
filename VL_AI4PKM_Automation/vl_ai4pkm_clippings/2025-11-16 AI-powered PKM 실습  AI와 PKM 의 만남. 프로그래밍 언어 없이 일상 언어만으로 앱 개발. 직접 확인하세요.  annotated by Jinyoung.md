---
title: "2025-11-16T05:56:53-08:00 AI-powered PKM 실습 | AI와 PKM 의 만남. 프로그래밍 언어 없이 일상 언어만으로 앱 개발. 직접 확인하세요. | annotated by Jinyoung"
source: "https://readwise.io/reader/shared/01k9dqazxfcjd9qhbcd1vwq1e4/"
author:
  - "[[Catch Up AI]]"
published: 2025-11-05
created: 2025-11-16
description: "AI-powered PKM오늘은 지난 시간에 이어 수집된 정보를 AI를 사용해서 가공하고 활용하는 방법을 실습해 봅니다.영어 콘텐츠를 한국어로 바꾸고 하루 동안 수집하고 공부한 내용을 가공하고 그 정보들을 기반으로 SNS 에 올릴 글을 자동으로 생성하는 과정을 실습합니다.하나의 PKM 어플리케이션을 만드는 겁니다.이 모든 과정을 프로그래밍 언어 전혀 사용하지 않고 인간의 일상 언어로만 만듭니다.🤖 AI와 PKM이 만나면?💬 일상 언어만으로 앱 개발 가능💻 프로그래밍 언어는 1도 필요 없습니다.👀 직접 확인하세요.PKM 이 AI 와 만난 후로 가능해진 이야기 입니다.어플리케이션 개발의 영역이 크게 확장 되었습니다.여러분 눈으로 직접 확인하고 프로그래밍 언어 없이 프롬프트 만으로 어플리케이션을 개발해 보세요.영상에 나오는 Link들Jin's Second Brain Bloghttps://publish.obsidian.md/lifidea/Publish/Homepage7-2. Field Guide for AI-powered PKM (실습)https://publish.obsidian.md/lifidea/Publish/AI+for+PKM/7-2.+Field+Guide+for+AI-powered+PKM+(%EC%8B%A4%EC%8A%B5)이전 영상https://youtu.be/Ku4_N4b78rcObsidian 설치https://youtu.be/aM7Ensrqurs?si=NxkazbiNRSpX691RGemini CLI 설치 https://youtu.be/MXyGrnIfTE0?si=uoDzRqKdBTpy42pu"
tags:
  - "clippings"
---
안녕하세요. 오늘은 본격적으로 PKM 실습을 진행하도록 하겠습니다. 먼저 이 그림을 보고 난 다음에 실습을 진행하면 훨씬 더 이해하기 쉬울 거예요. 실습을 진행하면서 자연스럽게 지난 시간에 한 일을 복습할 기회도 있을 겁니다. 우선 이 그림부터 이해하도록 하겠습니다. 이번 실습의 목표는 Daily Ingestion and Roundup (DIR) Workflow 를 한번 따라해 보는 것이다라고 되어 있죠. PKM 관련 콘텐츠를 블로그에서 읽어서 저장하는 것은 지난 시간에 완료를 했습니다. 그 이후에 이를 바탕으로 Daily Roundup 과

Topic Index 페이지를 생성하고 마지막으로 이 지식을 활용하는 몇 가지 사례까지 한번 살펴보는 것이 이 페이지에서 다루는 내용입니다. 그림을 보면 지난주에 한 일은 바로 이 맨 왼쪽에 해당되는 부분입니다. 그리고 오늘 배울 내용이 오른쪽에 해당되는 내용을 배울 거고요. 그럼 지난주에 한 내용을 간단히 살펴보도록 하겠습니다. 이게 바로 Obsidian Vault 인데요. 여기에서 요 부분을 지난주 했죠? 데이터를 읽어서 Ingest/Clippings 폴더 아래에 저장하는 것까지

했습니다. 데이터를 읽는 방법은 간단하게 이렇게 처리를 했죠. 이것은 Startup 425라는 단체인데요. 제가 요즘에 이 단체에서 하는 행사에 자주 참여를 하게 돼서 관심을 갖게 된 그 NGO단체입니다. 여기에서 이 내용에 대해서 제가 보관을 하고 뭔가 좀 공부를 하고 싶다라고 하면 여기에 있는 옵시디안 아이콘을 클릭만 하면 되죠. 여기서 Add to Obsidian button 을 누르면 이 내용이 자동적으로 이렇게 나의 로컬에 저장이 되게 되는 겁니다. 오른쪽에 보시면 이렇게 글자들과 함께 그림도 같이 저장이

됐습니다. 이 웹페이지에 있는 내용을 아주 손쉽게 나의 Obsidian Vault 로 저장하는 것까지 지난주에 완료한 내용입니다. 여기에서 이 맨 왼쪽에 있는 부분은 데이터를 수집하는 과정을 다루는 겁니다. 바로 이 두 가지 중에 저희들은 요 부분을 세팅을 했고요. 웹페이지에 있는 내용을 긁어 와서 특정 폴더에 저장하는 것을 진행을 했습니다.이 데이터를 가지고 오는 방법은 여러 가지가 있겠죠. 그중에 맨 위에 있는 것은 이 저자가 사용하는 방법 중에 하나인데요. limitless 라는 이런 도구가 있어요. 어떤 것이냐

하면은 limitless AI 라고 검색을 하시면 요렇게 뭔가 도구들이 나오죠.이 이 펜던트를 차고 다니면 내가 다른 사람과 대화하는 내용을 하루 종일 녹음을 하게 됩니다. 그래서 다른 서버로 보내서 그것을 AI로 활용할 수 있도록 만드는 도구예요. 그래서 이 저자는 데이터를 수집하는 방법이 리밋리스로 수집하는 것도 있고 이렇게 웹 정보를 가지고 와서 수집하는 것도 있고 그 외에 다른 많은 방법들이 있을 수 있을 겁니다. 하여튼간 이 왼쪽은 데이터를 수집하는 것과 관련된 내용이고요.

우리는 지금 요 방법만 가지고 실습을 하고 있습니다. 지금 limitless를 사용해서 정보를 수집하고 있지 않기 때문에 요 부분은 신경 쓰지 않으셔도 되고요. 여기 웹페이지 정보를 수집하는 Ingest/Clippings 부분만 생각을 하시면 됩니다. 그 다음에는 EIC라는 걸로 넘어가죠. 이게 무엇인지는 바로 요 아래 단원에서 살펴볼 거고요. 이 오른쪽은 무엇이냐 하면 수집된 정보를 바탕으로 AI를 활용해서 어떤 일을 하는 겁니다. AI와 같이 일을

하기 위해서 가장 기본이 되고 가장 중요한 것은 프롬프트이죠. 그래서 여기서 얘기하는 EIC 이것은 프롬프트입니다. 그리고 여기서 얘기하는 PLL도 프롬프트이고요. 이 EIC는 웹에서 이 Obsidian icon 을 사용해서 수집한 정보가 있는 이 Clippings 폴더 아래에 있는 그 내용들을 관리하기 위한 프롬프트이고요. 이 PLL은 Limitless 도구를 사용해서 수집된 정보를 처리하기 위한 프롬프트입니다. 두 번째 단계부터는 AI와 소통하는 것이기 때문에 프롬프트에 대한 정보가

많이 있습니다. 오늘은 그것을 중심적으로 다룰 거고요. 그리고 이 프롬프트들을 자동화하는 스크립트 부분도 있어요. 오늘은 매뉴얼로 이 프롬프트를 돌려보는 그 방법을 살펴볼 거고요. 스크립트 관련된 내용은 아마 다음 시간에 살펴볼 수 있을 겁니다. 그래서 여기에 있는이 그림은 Daily Ingestion and Roundup (DIR) workflow 를 나타내는 거고요. 데이터를 수집하는 단계, 이것을 처리하는 단계, 그것을 자동화하는 단계, 그리고 이 자동화를 통해서 처리된 데이터를 내가 원하고자 하는 일에 활용하는 단계. 이렇게 세 가지 혹은 네 가지 단계를 나타내는 겁니다.

그럼 먼저 가장 처음 나오는 Daily Ingestion and Roundup (DIR) 페이지를 보도록 하겠습니다.이 Daily Ingestion and Roundup (DIR)은 줄여서 DIR 이라고 얘기를 하는데 여러 가지 프롬프트들을 Sequential 하게 실행시켜서 매일 반복되는 그런 프로세스를 처리할 수 있도록 한 겁니다. 여기 나오는 각 항목들은 다 프롬프트를 말하는 거예요. 이 DIR도 보시면 이게 프롬프트입니다. 그래서 AI에게 지금 인풋 데이터는 어떤 것이고 내가

원하는 아웃풋은 무엇이고, 이런 일을 좀 해 줘 라고 하는 일에 대한 지시를 내리는 것입니다. 그래서 DIR은 그 프롬프트를 말하는 것이고요. 여기 나오는 PPP도 프롬프트를 말하는 것입니다. AI에게 전달될 내용이에요. 인풋은 무엇이고 아웃풋은 무엇이고 네가 할 일은 이런 일들이다 라고 AI에게게 지시를 내리는 거죠. 이 프롬프트들은 지난 시간에 다운받은 여기 Vault 를 보시면 AI4pkm\_vault 라고 되어 있죠. 여기에서 Settings 로 가서 Prompts 를 보시면 여기에 다 있죠.

EIC 같은 경우에는 여기에 있고요. 이게 바로 EIC 프롬프트입니다. 여기서 클릭을 하시면 웹에 있는 이 내용과 로컬에 다운받은 이 EIC 내용이 같은 거죠. 기본적으로 이AI-powered PKM 실습은 데이터를 수집하고 그 수집한 데이터를 AI를 활용해서 처리하는 일을 하는 것이고요. AI를 활용하기 위해서는 프롬프트가 가장 중요한 것이고 그 프롬프트를 사전에 이렇게 정의해서 이 프롬프트를 사용해서 AI를 이용하는 바로 그런 컨셉입니다. 그럼 이 그림과 관련된 이해는 이 정도로

하면 될 거 같고요. 본격적으로 실습에 들어가도록 하죠. 일단 PKM 관련 블로그 몇 가지를 추려서 이 웹 Clipper 로 가져오는 일을 할 겁니다. fortelabs.com으로 한번 가죠. 예, 여기 여러 가지 Article들이 있는데요. Web Surfing 을 하다가 내가 좀 더 공부하고 싶은 내용이 있으면 그 부분에 대해서 Clipping을 하면 됩니다. 여기에 이 'a guide to the cloud 4 and chatGPT 5 system prompts' 라고 되어 있네요. 요 내용이 좀 흥미로우니까 이 내용을 좀 공부를 하고 싶다 하면은 이것을 나의 로컬로 불러올 수 있죠. 여기서 이 Obsidian icon 을 클릭하면

그 내용이 여기에 이렇게 저장이 되게 됩니다. 하나만 더 해 볼까요? 여기서 Mushroom 얘기가 있죠. 버섯에 대해서 관심이 많은데 이번 주에 시간이 있으면 한번 이 근처에 있는 산으로 송이 버섯을 따러 갈까 하는 계획도 있습니다. 버섯과 관련된 article 이 있으니까 요것도 좀 보고 싶은 마음이 들어서 클리핑을 하도록 하겠습니다. Add to Obsidian button을 누르면 이렇게 오른쪽에 보시면 저의 Obsidian vault 로 저장이 돼 있죠. 여기에 있는 파일들은 저의 로컬에 있는 파일들입니다. 그리고 다른 글을 한번 살펴볼까요? 일단 '미분과 머신 러닝'에 대해서 제가 만약에 공부를 하고

싶다라고 하면 이렇게 검색을 하죠. 그래서 요 내용을 한번 보도록 하겠습니다. 여기 '자동 미분과 계산 그래프' 요 웹페이지가 떴는데요. 요 내용을 한번 자세하게 공부하고 싶다라고 하면 이 웹페이지를 클리핑을 하면 되죠. 여기서 Obsidian icon 을 누르고 Add to Obsidian button 을 누르면 이 페이지가 저의 로컬에 이렇게 저장이 됐죠. 오늘 제가 공부한 내용들은 이 네 가지가 로컬에 저장이 되어 있습니다. 오늘은 이 네 가지를 활용해서 그 아래 실습을 진행하도록 하겠습니다. 첫 번째로는 영문

아티클을 한글로 번역해 주는 그런 프롬프트를 한번 진행을 할 건데요. 클리핑한 내용을 EIC를 사용해서 영어를 한국어로 번역을 해 주는 그런 기능을 구현을 한 겁니다. Vibe Coding tool을 활용해서 AI로 로컬에 있는 파일을 관리하는 방식으로 설명이 되고 있습니다. 그래서 저 같은 경우에는 VS Code 와 GitHub Copilot 그리고 이 터미널에서 실행하는 Gemini CLI를 사용해서 AI를 활용하는데요. 요 부분을 한번 Vibe Coding tool 을 사용해서 실습을 진행을 하도록 하겠습니다. 이 화면은 아까 Obsidian Vault에서 보셨던 폴더와 파일들을 그대로 여기에서 불러온

거예요. 보시면 방금 전에 클리핑했던 그런 데이터들도 여기에 있는 것을 보실 수가 있죠. Vibe Coding 을 해 보신 분들은 아시겠지만 여기에서 그냥 Vibe Coding을 하시면 돼요. 지금 현재는 EIC라는 프롬프트를 실행을 할 건데요. 그 프롬프트는 여기에 있었죠. Settings 밑에 Prompts 밑에 EIC라고 하는 것이 있는데 이게 바로 AI에게 지시를 하는 프롬프트입니다. 프럼프트 내용이 무엇인지 한번 살펴보도록 하겠습니다. 다시 이 페이지로 가고요. 여기에서 Enrich Ingested Contents 를

클릭을 하시면 이렇게 프롬프트가 나오죠. 이것이 로컬에 다운받은 프롬프트와 똑같은 겁니다. Enrich Ingested Content 라고 해서 EIC 로 줄여서 말을 합니다. 인풋 데이터는 Target Note File 이 되고요. 그리고 긴 Articles 들은 Chunks 로 나누는 그런 작업을 할 필요가 있을 수도 있죠. 그리고 오리지널 콘텐츠는 문법에 에러가 있을 수도 있고요. 아니면 voice 를 텍스트로 만드는 데이터라면 그 보이스가 텍스트로 잘못 트랜스크립 (받아쓰기) 가 될 수 있는 거죠. 그 인풋 데이터에 대한 정보를 여기에 넣는 것이고요. 아웃풋 데이터는

Set per skill 이라고 되어 있고 Status가 있고 processed 라고 되어 있고 obsidian-yaml-frontmatter 이렇게 아웃풋이 된다고 나와 있네요. 이것은 어딘가에 이것과 관련된 지시 사항이 따로 있는 부분이 있는 것 같습니다. 요 부분은 나중에 따로 한번 살펴봐야 될 것 같아요. 그리고 아웃풋에 대한 또 다른 지시 사항은 요것이 있어요. source article이나 embed video에 대한 링크를 추가를 하라고 지시를 하고요. 첫 부분에 그 섹션에 대한 Summary 를

넣으라고 하는 지시가 있는 거죠. 그리고 formatting 이나 structure를 어 Improve 하라는 것이 있죠. 좀 더 보기 좋게 꾸미라는 말이에요. 그래서 메인 프로세스는 이런식이 될 겁니다. IMPROVE CAPTURE & TRANSCRIPT 는 ICT 라는 것이고요. 그리고 add summary for thread 있고 ENRICH USING TOPICS 라고 해서 AI에게 지시를 내리는 자세한 내용들이 있어요. 여기에 Clipping 한 것에 대해서 한국어로 번역을 해라 라는 내용도 있죠. 그리고 그 아래 지시 사항들도 있습니다. 요 부분은 한 글로 바꿔서 한번 살펴보도록 할까요? ICT 섹션은 잘리지 않고 완전해야 됩니다.

일반적인 실패 패턴은 요런 것들이 있어서 주의하라고 하는 것 같아요. 예방 조치는 요런 것들이 있어서 요것을 염두에 두고 일을 진행하라 라는 것이고요. 먼저 시작하기 전에 기사 길이를 확인하고 소스가 3,000 단어보다 큰 경우에는 Chunk로 나눠서 처리를 해라 라는 뭐 요런 지시사항들이 있어요. 그리고 품질 검증 부분도 명기를 해 놓았고요. 길이는 원본 소스와 비슷해야 됩니다. 잘림으로 인해서 30에서 50% 이상 짧아지지 않게 한다. 그래서 너무 많이 Summarize 를 하지 말라. 가급적이면

원본에 있는 내용들을 더 많이 보존하기 위해서 요 프롬프트가 들어가 있는 것 같습니다. 완전한 ICT를 완료할 수 없는 경우에는 뭐 이렇게 해라. 그리고 파일 이름 바꾸기는 뭐 요런 식으로 하고 서식 표준은 챕터의 경우 H3로 하고 요약을 우선으로 하고 뭐 요런 내용들이 기가 되어 있어요. 이런 프롬프트 내용이 바로 Enrich Ingested Content 즉 EIC인 것입니다. 그래서 오늘 첫 번째로 실습할 내용이 바로 이 EIC를

실행을 해서 영어 아티클을 한글로 요약해서 보여 주는 것을 실습을 해 보도록 하겠습니다. 여기서 gemini 를 입력하고 Enter key를 치면 gemini가 실행이 되죠. gemini 설치가 필요하신 분들은 지난 영상을 참조해 주시길 바랍니다. 일단 이 바이브 코딩 툴에서는 두 가지 바이브 코딩 툴을 사용을 할 수가 있어요. 먼저 오른쪽에 있는 이것은 GitHub Copilot 이라는 그런 Vibe Coding Tool 이고요.이 터미널에서는 Gemini 를 실행을 했기 때문에 Gemini CLI 라는 Vibe Coding Tool을 사용을 할 수가 있습니다. 여기서 EIC라는

프롬프트를 실행하는 실습을 해 보도록 하겠습니다. 실행하는 방법은 간단한데요. 아까 웹에서 다운 받았던 이 파일이 있죠? 이 이 파일을 현재 영어로 되어 있는데 이 영어로 된 것을 한국어로 바꿔 보도록 하겠습니다. 일단 요것을 비교를 하기 위해서 제가 Copy 를 하나 만들도록 할게요. 요렇게 하면은 Copy가 만들어졌죠. 이 Copy는 내버려 두고 아래에 있는 요것을 EIC를 돌려서 한국어로 번역을 하도록 하겠습니다. 파일 이름을 복사를 해서요. Gemini CLI에서 EIC4... 이

파일 네임을 클릭을 하면이 진행이 돼요. 한번 보도록 하죠. 먼저 이 EIIC 파일을 읽죠. 그 다음에 대상 파일을 읽습니다. 그러면 이 대상 파일을 EIC에서 지시한 대로 처리를 하겠죠. 그 AI에게 일을 지시를 하는 겁니다. 그래서 지금 번역 작업을 하고 있는 것 같아요. 그래서 뭔가 일을 처리를 한 것 같고요. 이 change를 Apply 할 것이냐고 물어보는데 Apply를 하죠. 그러면 이런 식으로 영어가 한국어로 번역이 됐죠. 요 내용을 한번 비교해서 보도록 하겠습니다. 이렇게 해서요,

왼쪽에는 원본을 놓고 오른쪽에는 Copy 놓으면 이 Copy는 영어 그대로 되어 있죠. 오른쪽에 있는 이 내용을, 보시다시피 왼쪽에서 처럼 이것을 한국어로 번역을 했어요. 그리고 한국어로 번역을 할 때 그대로 번역을 한 것이 아니라 요약을 해서 번역을 했고요. 관련된 그런 자료의 링크들도 아마 여기 있을 겁니다. 바로 여기에 EIC에서 지시한 대로 일을 처리를 한 것입니다. 그럼 요 부분을 닫고요. 하나만 더 실행을 해 보도록 하죠. 여기에는 지금 두 가지 Vibe Coding Tool 이 있다고 했죠. Gemini CLI와 GitHub Copilot.

그래서 이 GitHub Copilot을 사용을 하셔도 돼요. 그래서 GitHub Copilot에서 저는 GPT 5를 선택을 해서 사용을 하고 있는데요. 똑같이 사용을 하시면 됩니다. 여기서는 Startup 425, 이 아까 Clipping 했던 요 웹사이트를 EIC를 사용해서 번역을 해 보도록 하겠습니다. 이것도 마찬가지로 Copy를 하나 만들어서 비교를 할 수 있도록 할게요. 여기에서 이 파일 제목을 복사를 한 후에 똑같이 여기에다가 'EIC for' 하고 파일 네임을 주시면은 영어를 한국어로 번역을 해 줄 거예요. 지금 이

툴에서는 GPT 5가 사용이 되고 있죠. Gemini CLI에서는 Gemini AI가 사용이 됐고요. 여기에서는 GPT 5를 사용을 하고 있습니다. 이 GitHub Copilot 을 사용하면 다른 AI를 선택해서 사용할 수도 있어요. 그래서 요것을 지금 제가 실행한 이유는 여기에서는 이런 스텝들을 자세히 볼 수가 있거든요. 이 지시를 했더니 얘는 일을 세 가지로 나눴어요. GPT5는 추론 모델이거든요. 그래서 나름대로 추론 방법론을 활용해서 추론을 하면서 계획을 세우고 있는 겁니다. 첫 번째는 Clipping한

웹페이지에서 가져온 이 데이터를 읽었고요. 나름대로 그 다음에 계획을 또 세웁니다. 이게 지금 GPT 5가 계획을 세우고 있는 부분이에요. 참조 삼아서요 부분도 어떤 것인지 한번 살펴보도록 할게요. GPT 5가 나름대로 세운 계획을 여기 복사해서 살펴보도록 하겠습니다. 그러니까 이런 식으로 지금 생각을 하고 있어요.이 웹사이트를 EIC를 준수하는 한국어 구조로 변경하는 작업을 진행 중입니다. 얘가 EIC 프롬프트를 읽었죠. 그래서 자기가 지금 현재 이 작업을 하고 있다고 스스로 생각을 하는 것입니다. 이미지 링크는 그대로 유지해야 합니다. 이것도 EIC에서

지시한 내용이죠. 그리고 여기에서 제공하는 지역 학습 및 네트워킹과 같은 서비스 그리고 예정된 행사 샘플과 블로그 내용을 요약해야 합니다. 자기가 해야 할 일이 무엇인지 지금 정리를 한 겁니다. 이게 지금 AI가 답변한 것은 아니에요. 추론하는 과정을 공유를 한 거죠. 그래서 얘는 또 어떤 계획을 세웠냐면 웹사이트 콘텐츠의 구조를 변경을 해야 된다고 그 계획을 세웠어요.이 상단의 이미지와 가능하면 블로그 이미지 하나를 포함시키고 행사 링크 목록을 간략하게 정리를 할

거다라고 계획을 세웠죠. 그리고 EIC 규정에 따르면 여기서 그 지시한 프롬프트 규정에 따르면 원본 콘텐츠와 길이는 비슷하게 유지해야 됩니다라고 되어 있고요. 여기에서 지시한 내용을 명심하면서 얘가 일을 진행을 지금 하고 있는 중이죠. 그래서 요런 식으로 자기 나름대로 계획을 세웠어요. 그래서 구체적으로 웹사이트 구조는 이런 식으로 구현을 해야 되겠다 라고 플랜을 짠 겁니다. YAML은 그대로 유지하고 요약에는 한 줄짜리 글리 기호를 만들겠다. 뭐 캡처 및 대본 개선 섹션으로 넘어가서

동부 지역 기업과 연결이라는 개요로 시작하겠다. Startup 425는 워싱턴주의 Westside는 시애틀이거든요. East side가 IT 지역인데 이 지역을 기반으로 활동하는 NGO 단체예요. 기업가들 즉 Small Business owner들이나 Startup Founder 들을 대상으로 이벤트를 진행하는 곳인데요. 여기 동부 지역은 그 이스트 사이드를 말하는 겁니다. 그래서 그 내용을 일단 이 페이지 내용을 읽어 와서 위위에서 지시한 대로 이런 식으로 일을 수행을 하겠다라고 계획을 세운 겁니다. 그리고 콘텐츠

구조 마무리는 길이를 효율적으로 관리하기 위해서 모든 항목을 유지하는 대신 몇 개의 항목만 유지할 계획입니다. 뭐 모두 유지하지 않고 몇 개만 줄여서 유지를 하겠다는 거네요. 뭐 이런 식으로 자기가 나름대로 계획을 세운 겁니다. 지금쯤이면 일을 다 처리를 했겠죠. 이것은 추론 과정을 공유를 한 것이고요. 제가 아까 Gemini CLI에서 실행을 한 다음에 이 GitHub Copilot에서 GPT5로 다시 실행한 이유는 이렇게 자세하게 추론 과정을 한번 보여 드리고 싶어서 진행을 했습니다. 그래서 얘는 이 파일에 47개의

새로운 라인을 추가를 했고요. 101개의 라인을 삭제를 했습니다. 작업을 다 완료를 했죠. 작업을 이렇게 완료를 했다라고 보고를 하는 겁니다.요 보고 내용도 한번 살펴보도록 하죠. 요것은 추론 과정이 아니라 일을 다 처리하고 난 다음에 user 에게 이렇게 일을 처리했다 라고 보고를 하는 겁니다.이 Clipping 한 내용에 대해서 EIC를 적용해서 간결한 그런 한국어로 구조화하고 원본본 미디어와 주요 링크들을 보존했습니다 라고 하는 것이고요. Actions Taken 즉 취해진 조치는 이런 내용들이 취해졌다 라는

것이에요. 한국어도 요약을 했고, 여섯 개의 장으로 구성된 캡처 및 기록 개선 섹션이 있다. 이것은 ICT에서 지시한 내용들을 수행을 했다 라는 것을 이야기를 하는 것이고요. 요런 식으로 파일들을 처리를 했다 라고 되어 있어요. 그리고 긴 이벤트 섹션을 하이라이트 목록으로 압축해서 읽기 쉽고 EIC 스타일에 더 가깝게 메모를 작성하면서도 RSVP 링크와 날짜를 유지했습니다 라고 되어 있네요. 바로 여기에서는 이 단체가

진행하는 행사 정보도 있고요.이 행사 정보에 내가 참가 신청을 하는 RSVP 링크도 있어요. GPT5는 이 링크가 중요한 링크라고 생각을 해서 보존을 했다라고 여기 보고를 하는 겁니다. 처리된 내용을 한번 보도록 할게요. 여기에서이 파일을 요런 식으로 처리를 했죠. GitHub Copilot 을 사용하면 여기에 사각형 안에 점이 있는데요. 이것은 지금 어떤 내용이 변경됐다는 것을 의미합니다. AI가 변경한 내용을 유저가 Accept 를 할 것인지 아니면 취소를 할 것인지를 선택을 하실 수가

있어요. 여길 보시면은 아티클이 영어로 되어 있었는데 영어로 된 내용을 맨 처음에 Summary 한 내용이 표시가 되죠. 이것은 바로 EIC에서 지시한 내용이죠. 여기서 Summary 부분을 처음에 표시를 하라고 지시를 했습니다. 그 지시대로 따른 것이고요. 그리고 개요와 대상, 지원 영역까지 Summarizer 를 했고요. 그 다음에 영어로 된 부분은 다 이렇게 지웠어요. 빨간 부분은 얘가 삭제를 했다는 것이고요. 이것 대신에 이 빨간 부분을 요약을 해서 정리를 했다는 내용이고요. 그 다음에

3번은 아래에 있는 내용들을 없애고 이 Upcoming Events 들을 요약을 해서 여기 정리했다는 내용이죠. 한글로 번역을 한 다음에요. 그래서 아래 보면 이렇게 빨간 것의 내용이 많은데요.이 AI는 EIC에 있는 Prompt 에서 지시한 대로 나름대로 판단을 해서 이것을 줄인 겁니다. 줄였는데 여기서 RSVP는 중요하니까 이 링크는 보관을 하겠다 라고 아까 여기 보고를 했죠. 그래서 그 내용대로 여기 그 RSVP 링크는 보관을 한 것을 보실 수가 있어요. 그 이외의 내용들은 그냥 행사 내용을 요약을 한 거죠. 연말

Small Business Community 행사다. 뭐 며칠 날 어디에서 열린다. 그 행사 제목은 무엇이다. 이렇게 핵심 내용만 요약을 했습니다. 그 밑에도 마찬가지로 영어로 대한 부분들을 요약을 해서 위에 그 표시를 했고요. AI가 판단해서 EIC에서 요청한 내용을 반영하는 그런 작업들을 했고요. 그러면 이 내용들이 제대로 작업이 된 것 같으니까 저는 Keep button을 눌러서 이것을 보관을 하도록 하겠습니다. 그러면 지워진 부분들이 없고 AI가 한국어로 요약을 한 그런 내용만 여기 나와 있죠. 이것이 바로

EIC 프롬프트를 실행을 한 것입니다. 여기에 이미지 링크도 이렇게 있죠. 그래서 MD 파일로 보면 이 이미지가 출력이 되는 것을 보실 수가 있어요. 이것은 Vibe Coding Tool로 지금 작업을 한 것이고요. 이 똑같은 내용이 Obdisian 에도 반영이 되어 있을 겁니다. 왜냐하면 똑같은 폴더를 지금 참조를를 하는 것이니까요. 그럼 Obsidian 을 한번 보도록 하죠. 여기에서 지금 이것들을 돌렸죠. 그래서 방금 전에 Startup 425 부분을 돌린 것을 보면 이렇게

이미지가 나와 있고요. 그리고 그 내용들을 한국어로 요약을 해서 정리한 것들을 보실 수가 있어요. 이 Copy 는 웹에서 가져 온 원본이죠. 웹에서 가져 온 원본을 그대로 내가 뭐 보관할 필요는 없죠. 이것을 내가 활용하기 위해서 필요한 부분만 이렇게 요약을 할 수도 있고요. 그리고 다른 나라의 언어로 되어 있으면 내가 사용하는 언어로 이렇게 바꿀 수도 있습니다. 이 바꾸는 작업은 Prompt를 사용해서 작업을 한 거죠. 이 EIC, 미리 작성된 이 Prompt 에 그런 일을 하도록 잘 그 정리가 되어

있어요. 그래서 만약에 여러분들이 Web 에서 캡처한 내용, 아니면 다른 데서 가지고 온 내용을 여러분들이 원하는 대로 AI를 사용해서 정제를 하고 싶다면 여기에 있는 이 EIC Prompt 를 수정을 하던가 아니면 새로운 프롬프트를 만들던가 해서 그 작업을 실행을 시키면 되죠. 그 과정이 바로 7-2. Field Guide for AI-powered PKM (실습) 에서 첫 번째로 실습을 진행한 그런 내용입니다. 여기에 섭취 및 강화라고 나오네요.

이게 원래 영어로 되어 있는 거라서 좀 번역이 이상했던 것 같네요. 예. Ingestion & Enrichment 부분을 그 실습을 진행을 한 것이고요. 다른 것들도 같은 방식으로 실습을 진행을 하시면 됩니다. 그리고 처음에 시작할 때 말씀드렸던 것처럼 여기에는 세 가지 혹은 네 가지 단계로 나눌 수가 있는데 처음에 데이터를 수집하는 단계가 있고요.이 수집한 데이터를 처리하는 단계가 있죠. 이 수집하는 단계가 Ingestion 이라고 보면 되고 처리하는 단계 중에 하나가 Enrichment 라고 이야기를 할 수가 있죠. 이것을 지금

현재 우리는 매뉴얼로 돌렸고요. 이 매뉴얼로 프롬프트를 돌리는 것 대신에 스크립트를 사용해서 자동적으로 이 프롬프트를 돌릴 수도 있습니다. 그리고 프롬프트를 사용해서 자동적으로 이 프롬프트들을 돌릴 수가 있고요. 예를 들어서 이 프롬프트를 사용해서 처음에는 이 프롬프트를 돌리고 두 번째는 저 프롬프트를 돌리고 뭐 이런 식으로 AI에게 지시를 하는 방법도 있을 수가 있죠. 그리고 Shell Script 를 사용해서 각각의 프롬프트들을 순서대로 돌리게 할 수도 있고요. 오늘은 매뉴얼로 돌리는

방법을 알아보았고요. 다음 시간에 아마 기회가 되면 Script 를 사용해서 이러한 일들을 자동화하는 그런 방법을 공부하게 될 것 같습니다. 이것 이외에 그 다음 내용은 Daily Report 라는 부분이 있는데요. 이것은 GDR이라는 프롬프트를 돌리는 것을 말합니다. 똑같은 방법으로 돌리시면 돼요.이 GDR 프롬프트는 무엇인지 한번 살펴보도록 하겠습니다. 여기에서 프롬프트가 저장된 부분으로 한번 가죠. 여기서 GDR이라는 프롬프트가 있죠. 여기서 이 프롬프트를 한번 분석을 해 보도록 하겠습니다.

Google Translate 로 가서 한국어로 같이 보면서 분석을 하면 더 수월할 것 같아요. 예전에 어떤 Application 을 개발할 때는 Development language 를 사용해서 Scripting 을 했죠. JAVA를 사용하든가 Python 을 사용하든가. 그래서 그때는 그 스크립트를 분석하고 수정하고 만드는 것이 주된 일이었습니다. 하지만 AI 시대는 Vibe Coding 으로 그런 일들을 할 수가 있죠. 지금 현재 작업은 코딩 부분은 아니지만 이것도 AI와 함께 작업을 할 수가 있습니다. 그렇기 때문에 이 프롬프트를 분석하는 것이 예전에

Traditional 한 Application Development에서 Script 를 분석하는 것과 거의 비슷한 작업입니다. 프롬프트를 어떻게 잘 만드느냐에 따라서 결과가 달라지니까요. 지금 이것은 블로그를 운영하고 있는 진 님이 만든 프롬프트이고요. 그것을 참조해서 실행을 하고 있는 것이고요. 여러분들이 이것들을 업데이트를 해도 되고 여러분들 나름대로의 프롬프트를 만들어서 사용을 하셔도 됩니다. 이 Generate Daily Roundup (GDR)은 GDR이라는 약어로 사용을 하고 있고요. Prompt에 나와 있죠? 약어는 GDR이다라고 표시를 했습니다. 그래서 GDR이라고만 명령어에 명령을

해도 AI가 아 이것이 Generate Daily Roundup 이구나라는 것을 알 수가 있는 것입니다. 그래서 이 프롬프트를 찾아서 AI가 Read 를 할 수가 있는 것이고요. 여기에서는 여러 출처를 인용 마이닝 및 주제 연결을 통해서 통합하여 포괄적인 일일 요약을 생성을 합니다 라고 되어 있어요. 그리고 그 구체적인 내용들이 밑에 정리가 되어 있고요. 그럼 무엇이냐면 오늘 내가 공부한 내용들이 있죠. 오늘 공부한 내용들은 여기에서 이 날짜를 보면 알 수가 있어요. 아, 오늘은 하나, 둘, 셋, 넷, 다섯, 여섯, 여섯 개의 그런 주제에 대해서 혹은 웹페이지에 대해서 공부를

했구나 라는 것을 알 수가 있죠. 오늘 배운 내용들을 하나의 파일로 요약을 하면 나중에 그것을 참조할 때 더 쉽게 함께 참조를 할 수가 있겠죠. 바로 그 작업을 하는 겁니다. Daily 로 GDR 프롬프트를 돌려서 그날 공부했던 내용을 하나로 요약하는 파일을 만드는 겁니다. 그래서 그것을 지시하는 것이 바로 요렇게 되어 있고요. 입력 대상은 YYYY-MM-DD 를 참조를 해서 이것으로 시작되는 그런 파일들을 처리를 하는 거죠. 이걸 같이 보면서 작업을 하면 더 좋겠네요. 여기에서 이것은 예,

요걸 보면서 할게요. 그래서 보시면 YYYY-MM-DD 바로 오늘 날짜 혹은 다른 특정한 날짜를 지정해 주셔도 됩니다. 저는 조금 있다가 이 11월 5일에 해당하는 날짜를 얘한테 정리하라고 시킬 거예요. 그래서 입력 데이터는 이것이 되고요. 그리고 Journal 에서는 요것을 시작점으로 사용을 한다. 이 Journal이라는 폴더 아래에 있죠? 요것이 시작점으로 사용을 하고요. 이 날짜에 대한 Ingest/Clippings, Limitless, Lifelog files 를 참조를 한다라 고 되어 있는데 저희는 지금

Ingest/Clippings 폴더 하나만 있습니다. 이 블로그를 운영하시는 분은 이것 이외에 그 Limitless를 사용해서 데이터를 수집을 하고 그 수집한 내용을 Limitless라는 폴더 아래에 저장을 하고 있고요. Lifelog 라는 별도의 폴더를 만들어서 그 아래에 저장을 하고 있습니다. 여러분도 이 Ingest 아래 여러분 나름대로의 폴더를 만들어서 그날 그날 공부한 것들, 수집한 데이터들을 보관을 하셔도 돼요. 보관을 하실 때 이 Naming Convention 만 맞추면 이 GDR 프롬프트로 처리를 할 수가 있죠. 여기에

여러분들이 만든 폴더만 추가를 해 주시면 됩니다. 그래서 파일들은 해당 날짜에 대한 Apple Note 도 사용 가능하고, 요거는 지금 현재 우리들은 가지고 있지 않은 거죠, 파일이 없는 경우에는 Journal Template을 사용한다 라고 되어 있고요. 이 Journal Template 이 따로 있을 겁니다. 아웃풋은 이렇게 되는 거죠. AI 폴더의 RoundUp folder 에 오늘 날짜가 되어 있고 그 아래에 파일을 생성을 한다라고 되어 있습니다. 바로 위에 있는 AI라는 폴더가 있죠. 여기에 RoundUp 이라는 폴더가 있고 그 밑에 오늘 내용을 요약을 하는 겁니다. 제가 11월 3일에 한번 테스트를 하느라고 돌렸는데 그 내용이

여기 되어 있는 거고요. 11월 5일 거를 돌리면 여기에 11월 5일로 시작하는 그런 파일이 생성이 되겠죠. 그리고 주요 프로세스는 이런 것이다 라고 AI에게 지시하는 내용들이 있고요. 그리고 주의 사항도 있고, 여기 중요하다고 아주 중요하다고 AI에게 강조를 하고 있죠. 타겟이 80%입니다. 그 오늘 공부한 내용에 최소한 80%는 커버를 해야 된다라고 하는 것이고 이것이 왜 있냐하면 AI 에게 지시를 내릴 때 지시가 여러 가지가 있으면, 예를 들어서 이 내용이 지금 현재는 한 여섯 일곱 개가

되죠. 이게 수십개가 될 수도 있죠. 그러면 그것을 AI가 요약할 때 많이 빼먹는 경우가 있을 수도 있을 거예요. 그런 것을 방지하기 위해서 최소한 80% 정도는 유지를 해라. 뭐 요런 것들을 프롬프트에 넣어 준 거고요. 인용 마이닝 기준은 뭐 요런 식으로 된다 라고 지시를 한 겁니다. 자세한 내용은 여러분들이 살펴보시길 바랄게요. 여러분들은 이것과 다른 구조로 운영을 하고 싶다고 한다면 수정을 해서 사용을 하시면 됩니다. 이런 JOURNAL ENHANCEMENT 규칙도 있고요. 간결하지만 포괄적이어야 되고, 기존 콘텐츠는 수정하지 말고, 왜냐하면 이 내용을 한 번 돌리는 것이 아니라,

예를 들어서 11월 3일 날 제가 여기 자료가 있는데 이것을 만들고 난 다음에 추가적으로 공부를 더 했어요. 그 내용을 Summarize 를 하고 싶으면 다시 이 GDR 프롬프트를 돌리면 됩니다. 하지만 기존에 있는 것을 덮어쓰면 안 되겠죠. 기존에 있는 내용을 덮어쓰지 말라는 내용을 여기에 지시를 한 것이고요. 각 콘텐츠에는 출처 노트 링크가 있어야 됩니다 라고 되어 있어요. 바로 그 출처는 이것들이 되겠죠. 여기에 대한 링크가 걸려 있어야 된다라고 지시를 한 것이고, 양방향 링크가 완벽하게

작성되었는지 확인해 라고 되어 있는 겁니다. 이것은 양방향 링크가 되어 있으면 Obsidian 에서 Graph View로 처리를 할 수가 있기 때문이에요. 이것은 완료한 다음에 제가 한번 보여 드리도록 하겠습니다. 요런 식으로 프롬프트가 작성이 되어 있는 거예요. 그러면 여기서 실행을 하도록 하죠. 아까 보았듯이 Gemini CLI에서 실행을 하셔도 되고요. 얘는 지금 뭔가 작업을 뭔가 하고 있나요? 여기에 지금 뭔가 작업을 아까 끝마치지 못한 게 있는 것 같아요. 그 작업을 지금 마저 하는 것 같고요. 이

GDR을 지금 실행을 할 차례죠. 이 블로그를 보면 요런 식으로 되어 있어요. 여기에서 'GDR for' 그리고 날짜를 정해 주죠. 이러면 자동적으로 얘가 GDR를 실행해서 주어진 날짜에 해당되는 Article 들을 모아서 요약을 해 줄 겁니다. 요것은 Gemini CLI로 해도 되고요. GPT 5로 해도 돼요. GPT 5로 먼저 하도록 하겠습니다. 여기서 'GDR for 2025-11-05' 저는 요렇게 명령을 내리도록 하겠습니다. 명령을 내리면 GDR

프롬프트가 실행이 되는 거죠. AI에게 전달해 돼서, AI가 요 날짜로 시작하는 내용들을 가지고 요약을 해서 AI 폴더 아래 있는 RoundUp 폴더 아래에 새로운 파일을 생성을 할 겁니다. 지금 얘가 계획을 세우고 있어요. GPT 5는 추론 모델이기 때문에 추론 하는 것을 여기 표시를 하고 있습니다. 추론 을 어떻게 했는지 한번 살펴보도록 할게요. 이것도 Vibe Coding 할 때 이 추론 과정을 살펴보는 것이 도움이 많이 돼요. 그런 습관을 기르면은Vibe Coding을 아주 수월하게 작업을 하실 수 있을 겁니다.

여기에서 Google Translate를 통해서 이 내용을 한글로 한번 바꿔서 보도록 하죠. 얘는 이렇게 계획을 세웠어요. GDR를 Run 하는데 2025년 11월 5일에 해당되는 것을 한다 라고 되어 있고요. 11월 5일 글로벌 일일 보고서를 실행해야 되고, 작업 공간에는 AI Roundup, Journal 등 여러 Directory가 있습니다 라고 되어 있고요. 여기서 5일에 해당되는 Roundup을 생성하겠습니다 라고 해서 Ingest 의 Clippings folder 에 있는 11월 5일자 항목을 확인하는 거죠. 이 확인할 항목 중 하나는 얘가 있다고 하네요. 그리고 다른 파일도 찾아보겠습니다라고 해서 자기가 지금 작업하는 내용을 계속

실시간으로 보고를 하는 거예요. GDR 프롬프트에 있는 내용들도 한번 자기 자신이 스스로 곱씹어 보기도 하고요. Roundup Source 통합 부분도 계획을 세웠네요. 자세한 내용은, 무슨 계획인지는 지금 일일이 다 살펴볼 필요는 없을 것 같고요. 이것은 AI가 나름대로 추론을 하는 겁니다. 그리고 Roundup 할 목록을 작성을 하고요. 자기 Tasks를 여기에 나누는 겁니다. Plan 을 하는 거죠. Roundup Contents 구조를 마무리하는 작업을 요런 식으로 할 거다. 2025-11-05 Roundup-Gemini와 같은

헤더를 포함하고 이전에 사용했던 스타일 형식을 사용해서 Journal 링크를 만들겠다라고 계획을 세웠습니다. 그래서이 계획대로 작업을 했는지 한번 살펴보도록 하죠. 이게 지금 계획을 세운 거고요. 그다음에 계획 세운 대로 작업을 하는 겁니다. 그리고 To do 를 하나 더 Add 를 했고 Clippings 에 있는 2025- 11- 05으로 시작하는 모든 MD 파일을 Search 를 했습니다. 여섯 개를 찾아냈고요. 여기 여섯 개가 있었나 봅니다. 그 여섯 개에 대해서 얘가 지금 작업을 하고 있습니다.

Roundup.md라는 파일을 만들어서 작업을 했어요. 그리고 두 번째로 작업할 때 어떤 추론 작업을 하게 되는 거죠. 어, 이것도 꽤 길게 지금 추론을 진행을 했네요. 어떤 추론을 했는지 한번 살펴보도록 할게요. 두 번째 추론을 한 것은 중복되는 파일들을 찾아냈네요. 이렇게 세 개의 중복되는 파일들을 찾아냈고요.이 중복되는 파일이 몇 개 있는 것 같습니다. AI가 스스로 뭐 이런 식으로 판단하요. 중복되는 것은 하나만 작업하면 되겠구나 뭐 이렇게 지금 나름대로 추론을 했고요. 그 다음에 소스를 통합할 계획을 하고 있어요. 이

문서에는 구체적인 내용이 없는 것 같습니다. 뭐 copy.md 파일을 검색해 봐야 될 것 같지만 굳이 그럴 필요는 없을 것 같습니다. 다음 출처를 목록에 추가하겠습니다라고 해서 요런 것을 이제 목록에 추가를 했네요. 처음에는 Copy가 있는데 Copy를 어떻게 처리할까 지금 그걸 고민했던 것 같아요. 근데 Copy는 별로 특별히 뭐 다시 볼 필요가 없겠다 라고 얘가 결정을 했고요. 그 다음에 Copy가 없는 다른 것들을 지금 살펴보고 있는 것 같아요. 이 Copy들을 무시한다 라는 것을 자기 나름대로 결정을 했죠. 그리고 일지 및 요약 정리를 요런 식으로 할

거다. Settings 의 템플릿에 있는 일지 템플릿을 참고해서 작업을 하겠다 라고 나름대로 생각을 했어요. 그 다음에도 어떻게 작업을 하겠다 라는 Plan들을 세웠고요. 이 Plan대로 작업을 진행을 했을 겁니다. 바로 요게 Plan이고요. 그 다음에 이 Plan대로 지금 작업을 쭉 하는 겁니다. 템플릿도 한번 보고 Settings 에 있는 그런 다른 내용들도 보고 하면서 지시한 내용들을 잘 이행하기 위한 과정들을 진행을 하고 있는 겁니다. 중간중간에 Todo 를 여러 가지 그 얘가 만들어 냈죠. 각 Todo 를 진행할 때마다 추론을 하는

겁니다. Planning 을 나름대로 지금 하는 과정은 여기에 보여집니다. 그래서 Gemini CLI로 실행하는 것보다 GPT5로 작업을 하면 이것이 어떤 흐름으로 작업을 하는구나 라는 것을 잘 아실 수가 있을 거예요. 지금 현재는 요 방법을 좀 제가 추천을 드리고요. Planning file structure 는 요런 식으로 될 거고 뭔가 작업을 다 완료를 한 것 같아요. 'I will generate...' 해 가지고 추론 과정을 다 끝마친 다음에 이런 식으로 작업을 할 것이다 라고 지금 보고를 하고 있죠. 보고하는 내용을 다시 한번 살펴보도록 하겠습니다. 여기에 나와 있어요. 자기 나름대로 추론

과정은 다 지금 마친 상태예요. 그래서 요런 내용으로 일을 진행을 하겠고요. Actions Taken은, 여태까지 한 일은 요런 것들이 있습니다. 그리고 작업 링크는 요렇게 되어 있고요. 요런 파일로 지금 생성이 될 거다라고 되어 있어요. 그리고 제목 날짜 태그는 뭐 요런 식으로 지금 작업이 되고 있고, 서문은 어떻고 이렇게 작업을 할 것이다 라고 보고를 하고 있습니다. 실제로 작업하실 때는 요 내용들을 잘 보시고 내가 원하는 대로 작업이 되고 있는지를 확인한 다음에 얘한테 작업을 하라고 뭐 지시를 내리는 경우도 있을

거예요. 근데 얘는 허락을 받지 않고 이 작업한 내용들을 그냥 처리를 했습니다. 왜냐하면은 여기에서 User 가 Accept 를 할 수도 있고 아니면 거절을 할 수도 있기 때문에 그래서 승낙받는 버튼은 따로 만들지 않은 거 같아요. 바로 요 부분이 만들어졌죠. 그래서 파일 제목은 11-05 Roundup-Gemini로 되어 있고요. 이것은 Gemini 로 실행한 것은 아니지만 이런 식으로 네이밍을 만들어라 라고 어딘가 프롬프트에서 지시를 했기 때문에 요런 식으로 이름이 만들어진 겁니다. 그리고 그 내용들은 여기에 있는

내용들이고요. 메타 정보를 여기에 넣었고 그 다음에 데일리 라운드업이 이렇게 정리가 되어 있습니다. 그래서 100% 커버했다는 것을 보고를 했고요. 요런 내용들도 다 프롬프트에 요런 식으로 그 내용을 정리하라 라고 지시를 했기 때문에 얘가 이렇게 정리를 한 것입니다. Daily Summary 로는 지역 창업 생태계로는 Startup 425에 대한 내용들이 있고 그리고 Claude 4, GPT-5 시스템 프롬프트 Insight 를 적용하는 것에 관련된 내용들도 있죠. 그리고 오늘의 실행은 가까운 Coworking 일정을 RSVP + 나의

프롬프트 프리셋을 정리해서 재사용했고 이게 바로 오늘 한 내용을 Summarize 한 것입니다. 그리고 그 아래에 각 항목들마다 Summarize가 되어 있겠죠. 그래서 Existing Topics 를 보는데 여기는이 Topics 디렉토리가 없다는 것인데, 이건 조금 후에 말씀을 드릴게요. 뭔가 구조가 변경된게 있는 것 같습니다. Topics to Create 은 얘가 이 Startup 425 에서 행사하는 것이 Bellevue, Kirkland, Issaquah, Redmond, Rendon 이 네 개의 지역을 기반으로 행사를 진행을 하거든요. 그것과 관련된 요약을 한 것입니다.

이것이 바로 워싱턴주의 Eastside 에 있는 IT로 유명한 도시들이에요. Westside에는 Seattle 이 있고, 이 도시들이 이제 IT로 유명하죠. Redmond 에는 마이크로소프트 본사가 있고요. 그리고 Nintendo 본사도 있습니다. Issaquah 에는 코스트코 본사가 있고 Renton 에는 보잉 공장이 있고 Kirkland, Bellevue에도 아마존이나 페이스북이나 Google 이런 회사들의 제 2 캠퍼스들이 있습니다. 그 내용들이 정리가 되어 있습니다. 그래서 오늘

내용들을 여기서 다 정리가 되어 있고요. 이것이 GDR 프롬프트를 돌린 결과입니다. 오늘 실습 부분은 EIC와 여기에 있는 GDR 프롬프트를 돌리는 것을 실습을 했고요. 방금 전에 보셨듯이 Gemini CLI 에서 돌려도 되고요. 다른 Vibe Coding Tool을 사용한다면 다른 Vibe Coding Tool을 사용해서 프롬프트를 돌려도 됩니다. 중요한 것은 이 프롬프트를 사용해서 AI를 활용하는 것이고 그 AI를 활용해서 내가 수집한 정보들을 가공을 하고 이용을 하는 것이 핵심이죠. 이 블로그에서는 이 GDR을 수행한

결과를 요런 식으로 보여 주고 있고요. 그 다음에 Topic Creation이라는 부분이 있는데 이것은 향후 주제별 지식 접근을 위한 Topic Note 를 생성하는 것을 이야기를 하는 것입니다. 이것도 마찬가지로 해당 프롬프트를 돌리면 되는 건데요. 이 링크는 지금 걸려 있지 않더라고요. 이 작업을 할 때는 이 부분도이 Structure 에 있었는데 나중에 업데이트 되면서 요 부분이 없어진 것 같습니다. 요 부분은 별도로 실습할 내용들은 없고요. 오늘 생성한 내용들을 Graph View 로 한번 확인 해 보는 것까지 한번 해 보도록 하죠.

이것은 Obsidian을 실행을 하시면 되고요. 이 Obsidian에서 왼쪽에 보시면 Open Graph View라는 것이 있어요. 이것은 Obsidian에서 제공되는 그런 기능인데요. 지금까지 작업했던 것들과 관련돼서 링크들이 다 연결이 되어 있죠. 아까 프롬프트에서 AI의 라운드업에 요약본을 만들고 이 요약본에 각 원본이 되는 그 출처의 링크를 걸어라 라고 되어 있죠. 그런 식으로 서로 간의 링크가 연결되 있으면 여기에서 그 내용을 보실 수가

있어요. 20251105가 오늘 작업한 거죠. 'Home Startup 425'는 요기에 링크가 걸려 있고요. 그리고 Roundup-Gemini 에 링크가 걸려져 있죠. 서로 이렇게 링크가 통해져 있는 겁니다. Journal 에 보면 20251105가 있죠. 여기에 서로 간의 링크들을 걸어 놓은 것이에요. 그래서 Graph를 보시면 오늘 작업한 내용에 대해서 1105 여기네요. 요 부분에서 이 링크가 걸려 있는 것을 확인을 하실 수가 있습니다. 그래서 보시면 'Home Startup 425'는 요 원문을 참조를 했고 그 다음에 Cloude 에 관련된 그런

내용은 Copy file을 참조를 했네요. 서로간에 이렇게 링크가 걸려져 있습니다. 이렇게 Graph View를 보시면 각 항목들마다 걸려져 있는 이 링크들을 확인을 하실 수가 있어요. 링크를 걸어 놓으면 나중에 관련된 내용을 참조해서 AI에게 다른 일을 시킬 때 훨씬 더 빠르고 정확하게 일을 처리를 할 수가 있습니다. 그럼 여기까지 오늘 실습을 진행을 했고요. PKM 어플리케이션에 대해서 설명되어 있는 것을 간단히 한번 살펴보도록 하죠. 다음 단계에는 이렇게 쌓여져 있는 지식을 내가 필요한 곳에 활용할 수

있는 그런 방법이 있겠죠. 바로 그래프에 보시면 이런 식으로 처음에 수집하는 단계, 데이터를 수집하는 단계는 요 부분을 지난 영상에서 다뤘고요. 오늘 다룬 영상은 여기에서 EIC를 사용한다든가 GDR을 사용해서 수집된 데이터를 가공하는 그런 과정을 공부를 했고요. 요때는 AI를 활용해서 그 데이터를 가공을 하고요. 지금 다루는 부분은 가공된 데이터를 바탕으로 내가 어떤 것에 활용을 할 것인가? 소셜 미디어에 올려도 되고요. 내가 따로 관리하는 방법대로 업데이트를 할 수도 있고요.

다른 추가적인 검색이나 디스커버리를 할 때 활용을 할 수도 있는 것입니다. 그 내용이 여기에 다뤄져 있는 것이고요.이 CTP 내용을 보시면은 관련된 내용을 소셜 미디어에 포스팅하는 그런 내용들이 있어요. 그래서 오늘의 Ingestion을 원문을 살려서 한글로 뭐 어디에 포스팅을 해라 라고 되어 있는 것이에요. 그래서 요 부분을 한번 볼까요? 이 프롬프트를 보면은 내용을 아실 수가 있을 겁니다. 요거를 복사를 해서 Generate Template

부분에서 한번 살펴보도록 하죠. 여기서는 소셜 미디어 쓰레드에 올리는 것을 그 샘플로 만든 것 같습니다. 그래서 입력은 lifelog 파일은 이 폴더 아래에 있고 Roundup 파일은 요 폴더 아래에 있고 Clippings 파일은 요 폴더 아래에 있고 이렇게 다양하게 나만의 데이터가 관리가 되고 있다면 그것들을 여기 명시하면 되죠. 우리는 지금 현재 이 Clippings 아래에 있는 내용들만 있고요. 여러분들은 다른 폴더를 만들어서 나름대로 다른 데이터를 관리를 하셔도 됩니다. 그러한 것들이 이제 입력 데이터가 되고요. 출력 데이터는 AI의 Sharable 디렉토리의 파일로 이제 이런 식으로

저장이 될 거예요. 그래서 Thread에 올라갈 그런 내용으로 여기에 정리를 할 거고요. Thread는 Thread당 최대 1000자 이내로 그 내용을 작성하라는 그런 명령어가 여기 있는 거고요. 출력 노트당 최대 다섯 개의 Thread를 만들 수 있다 라고 지시를 한 겁니다. 그래서 어떤 식으로 일을 하냐 하면 주로 콘텐츠를 분석해서 대상 파일을 읽고 공유 가능한 통찰력 인용문 및 Story 를 파악하고 콘텐츠 유형을 결정하고 뭐 요런 것들을

해라 라고 지시를 하고 있고요. 그 다음에 Thread 를 생성을 하는 거죠. 형식 및 Sourcing 은 요런 식으로 되고요. 쓰레드 길이 제한에 대해서 이야기를 하고 있고요. 출처도 표시를 하고 출력물 구성은 요런 식으로 진행을 해라라는 Prompt가 만들어져 있습니다. 요것을 돌리면 오늘 한 내용을 쓰레드에 올릴 수 있게끔 뭔가 작업을 하겠죠. 아까 GPT-5가 작업한 내용은 저장을 했고요. 요것까지 한번 해 보도록 할까요? 블로그에 이렇게 되어 있네요. CTP based on today's ingestion, 정확하게 하기 위해서 날짜를 주고 싶네요. 이번에는 GPT 5가 아닌 Gemini CLI를

한번 사용해 보도록 하겠습니다. 저는 좀 더 확실하게 하기 위해서 날짜도 정해 줄게요. 2025 - 11-05라고 날짜를 주고 명령을 내리도록 하겠습니다. Gemini가 이 작업을 하겠죠. Thread 에 올릴 그런 내용을 오늘 제가 공부한 것을 요약해서 만들어 줄 겁니다. 얘가 요 파일에 대해서 뭔가 작업을 하겠다고 하는 것 같은데 Yes 를 할게요. 그래서 뭔가 작업을 한 것 같네요. AI 폴더 아래에 Sharable folder 가 없는 것 같아요. 그래서 이것을 생성을 하겠다 라고 합니다.

보시면 AI의 아래에 있는 Sharable 폴더에 그 결과를 넣으라고 했는데 보니까 AI 폴더 아래에 Sharable 폴더가 없으니까 자기가 만들겠다고 하죠. 알아서 다음 일을 AI가 결정을 합니다. 만들라고 하고요. 그럼 여기에는 Sharable 이 만들어졌고요. 그 아래에 제가 지시한 대로 Thread 에 올릴 그런 내용들이 들어 있는 파일을 만들겠죠. 뭔가 Move 를 하겠다고 하는데 그 이전에 뭔가 여기 만들었었나 봐요. 이것을 Move 를 하라고 하고요. 보시면 얘가 바깥에 있는 것이 안으로 Move 가 됐습니다. 그래서 Complete 됐네요.

얘는 이 내용이죠. 아까 여기에 있는 이것을 SNS에 올릴 수 있는 버전으로 고쳤습니다. 이 쓰레드는 Claude 4와 Chat GPT 5에 유출된 시스템 프롬프트에 대한 가이드에서 생성되었습니다 라고 되어 있고요. Thread 1은 이것이고 Thread2는 이것이고 뭐 이런 식으로 내용들이 요약하게 되어 있네요. 그래서 요것은 이 블로그에서 얘기하는 데이터를 수집하고 그것을 가공하고 가공한 데이터를 내가 해야 될 일에 활용하는 그런 과정을 다룬 것이고 맨 마지막에 있는 Create Social Media Posting 부분을

실행을 한 것입니다.이 프롬프트를 참조를 하시고 여러분들은 여러분들이 하고자 하는 그런 일들에 대한 프롬프트를 만들어서 돌리시면 돼요. 기본적으로 여기서 공부하는 내용은 Personal Knowledge 를 어떻게 수집을 하고 AI를 활용해서 어떻게 가공을 하고 그것들을 어떻게 활용을 할 것인가라는 그런 Flow를 담고 있고요. 지금 오늘 한 일은 그런 내용들을 매뉴얼로 한번 돌려보는 실습을 한 것입니다. 그래서 끝까지 다 실습을 완료를 했고요. 두 번째 활용하는 방법은

Ad-hoc Research within PKM 이라고 되어 있는데 이건 ARP라는 프롬프트를 돌리면 이루어지는 일을 설명을 한 것입니다. ARP 프롬프트는 여기에 있고요. 그래서 요것이 어떤 일을 하는 것이고 실질적으로 어떻게 진행이 되는지는 여러분들이 직접 한번 실습을 진행해 보시길 바라겠습니다. 지난 시간에 이어서 오늘 7-2. Field Guide for AI-powered PKM을 끝까지 전부 다 다뤘습니다. 오늘은 매뉴얼로 이것들을 실행을 해 봤고요. 여기에 있는 내용들을 보면은 이것들을 자동화하기 위해서 어떻게 스크립트를 활용하는지에 대해서도 나와 있어요. 그리고 여기에 보시면은 Gemini CLI로 실행을 할 경우에

어떤 룰이 적용이 되는지 여기 명시되어 있고요. 그 다음에 Claude Code로 실행이 될 때는 어떤 룰이 적용이 되는지도 여기에 명시되어 있습니다. 요것도 여러분들이 한번 잠깐 보시면은 도움이 될 거예요. 지난 시간에 데이터를 어떻게 수집하는지에 대해서 다뤘고 오늘은 그 수집된 데이터를 어떻게 활용하는지에 대해서 배웠습니다. 그 활용하는 방법은 Prompt 를 사용해서 AI를 이용하는 방법인데 그것을 매뉴얼로 Run하는 방법을 배웠고요. 다음 시간에는 어떻게 자동화를 시키는지 그 스크립트

부분을 한번 배워 볼까 해요. 그것이 저의 계획이고요. 여러분도 혹시 뭐 또 궁금한 사항이나 다른 아이디어가 있으면 댓글에 남겨 주시면 같이 공부하는데 서로 도움이 될 것 같습니다. 그러면 오늘은 여기까지 진행하는 걸로 하고요. 다음 시간에 다시 뵙도록 하겠습니다. 감사합니다.

---